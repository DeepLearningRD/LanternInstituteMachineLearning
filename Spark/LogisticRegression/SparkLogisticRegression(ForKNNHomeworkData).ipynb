{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# NOTE: Please change the folder paths to your current setup.\n",
    "#Windows\n",
    "if sys.platform.startswith('win'):\n",
    "    #Where you downloaded the resource bundle\n",
    "    os.chdir(\"D:/\")\n",
    "    #Where you installed spark.    \n",
    "    os.environ['SPARK_HOME'] = 'D:/Spark/spark-2.0.1-bin-hadoop2.7'\n",
    "os.curdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a variable for our root path\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "#Add the following paths to the system path. Please check your installation\n",
    "#to make sure that these zip files actually exist. The names might change\n",
    "#as versions change.\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.3-src.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"SparkDecisionTree\")\n",
    "sc = SparkContext(conf=conf)\n",
    "st = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_csv('D:/SparkPythonDoBigDataAnalytics-Resources/Data/KNNHomeworkData.csv')\n",
    "s_df = st.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = s_df.columns[0:-1]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features,\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------+--------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+-------------------+-------------------+-----------+--------------------+------------+-------------------+--------------------+--------------------+------------+------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|                  V1|                V2|          V3|                  V4|          V5|                 V6|                  V7|                  V8|                  V9|                 V10|                 V11|        V12|                V13|                V14|        V15|                 V16|         V17|                V18|                 V19|                 V20|         V21|         V22|        V23|                 V24|         V25|                 V26|                 V27|                 V28|Class|            features|\n",
      "+--------------------+------------------+------------+--------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+-------------------+-------------------+-----------+--------------------+------------+-------------------+--------------------+--------------------+------------+------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|        -0.691658944|      -0.652790174| -0.04900457|        -0.992900695|  0.46128236|       -0.522475855|  0.6272028070000001|        -0.111401943|          -1.3914393| 0.46162393799999996| 0.29181491600000004|0.704503192|        1.169128347|0.17645116600000002|-0.54058574|        -1.803681321|-0.499998791| 2.0743655469999998|        -0.325631661|         0.186351111| 0.220324043| 0.962402938|0.211195548|        -0.448556585|-0.282338192|-0.05077270299999...|          0.41433691| 0.28916068300000003|    0|[-0.691658944,-0....|\n",
      "|  2.0579302719999997|      -0.327732268|-2.726418753|        -0.775523105| 2.454901259|        3.198474061|-0.45175185100000004|         0.733266592|         0.487501013|         0.029346206|         0.056698812|0.285314721|       -0.129718923|        0.663992258|1.199933318|-0.24914085600000002|-0.604949427|       -0.137701769|        -0.378915137|         -0.16967548| 0.256415813| 0.789349312|0.040268671|  0.7383569440000001|  0.28960664|          -0.0807948|0.002804994000000...|-0.06428453599999999|    0|[2.05793027199999...|\n",
      "|        -0.774356975|1.6386561130000001|-1.143462782|        -0.506452428| 0.236317234|-0.7994845490000001|         0.281419025|         0.678772878|-0.19594990199999998|-0.43922750200000005|        -1.256320981|0.319048816|0.35319487899999996| 0.9310757190000001| 0.47171188|        -0.224778667|-0.170518113|         0.18474036|-0.01386346600000...|        -0.077158009| 0.416903719| 1.257167917|0.008775485|         0.641379228|-0.663885773|-0.23436256100000002|         0.365418796|         0.261844345|    0|[-0.774356975,1.6...|\n",
      "|0.012269146000000002|       0.802440728| 0.990714175|-0.00421617900000...| 0.325431903|       -0.628611217|         0.809700649|-0.32906020199999997| 0.08821799400000001|        -0.888044203|-0.33362544899999996|0.456960711|        1.394666556|       -1.850907233|0.280437167|           0.4469045| 0.282740213|0.21640341899999999|        -0.018612329|         0.157034981|-0.154159352|-0.139544984|0.163508017|        -0.037868737|-1.277592149|-0.11742531199999999|         0.075383732|         0.045599363|    0|[0.01226914600000...|\n",
      "|         2.048757118|       -0.01534933|-1.054279461|          0.40416195|-0.050640316|       -1.110940843|         0.222672158|         -0.35087278|         0.430365568|0.042346534000000005|        -0.567484022|0.842665198| 0.7825716340000001|        0.138016905|0.008517719|-0.19729546399999998|-0.272864733|       -1.046588883|         0.068498003|-0.17893502100000003| -0.27345119|-0.617758768| 0.33589831|0.006684545999999...|-0.290008952| 0.19835491600000002|        -0.063349036|        -0.058476767|    0|[2.048757118,-0.0...|\n",
      "+--------------------+------------------+------------+--------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+-------------------+-------------------+-----------+--------------------+------------+-------------------+--------------------+--------------------+------------+------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembled_df = assembler.transform(s_df)\n",
    "assembled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = assembled_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=50).setLabelCol(\"Class\").setFeaturesCol(\"features\")\n",
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_summary = model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Class|prediction|\n",
      "+-----+----------+\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_summary.predictions.select('Class','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pandas_df = pd.read_csv('D:/SparkPythonDoBigDataAnalytics-Resources/Data/KNNHomeworkTestData.csv')\n",
    "pandas_df['Class']=0\n",
    "st_df = st.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurest = st_df.columns[0:]\n",
    "assemblert = VectorAssembler(\n",
    "    inputCols=features,\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_dt = assembler.transform(st_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary = model.evaluate(assembler_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_summary.predictions.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = test_summary.predictions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.to_csv('D:\\Output.csv',sep=',', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
